{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1TpD2a5QLeWlvOs_kh7LMJ8sckmwAvAm7",
      "authorship_tag": "ABX9TyNnPFUawqBjxo8QKiazwKRx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoonyerin/find_my_lover/blob/master/%EB%8F%99%EB%AC%BC%EC%83%81%ED%85%8C%EC%8A%A4%ED%8A%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <애인상 테스트를 위한 동물상 테스트>\n",
        "## 1. 실제 동물 classification에 사람 얼굴을 넣어 그 동물을 닮은 사람을 찾는 net을 구성하고자 함 \n",
        "###=> 걱정되는 부분: 그냥 인간과 종간 유사도가 비슷한 동물들만 치중되어 등장할 가능성이 있다. => 이걸 하나의 target으로만 예측되는 것을 막도록 cost function을 제작하거나 score값에 변형을 주는 방식으로 접근할 수도.\n",
        "## => 걱정했지만, 생각보다 문제없이 학습되는 것을 확인함. 배경의 영향을 안받도록 배경을 투명처리 또는 화이트 처리한 걸로 학습을 시키면 더 좋지 않을까 기대 중. \n",
        "## 2. 또는, 그냥 \"강아지상 연예인\" 이런식으로 스크레이핑 한 것들을 데이터 셋으로 사용하여 라벨을 가질 수도 있다.=>사람들의 보편적인 시각을 학습할 가능성이 높다. \n",
        "\n",
        "## 3. 둘 다 만들어보는 것도 재밌겠다.\n",
        "\n",
        "# <데이터>\n",
        "## 1. Kaggle animal data set\n",
        "## 2. target:\n",
        "badger\n",
        "bat\n",
        "bear\n",
        "bee\n",
        "beetle\n",
        "bison\n",
        "boar\n",
        "butterfly\n",
        "cat\n",
        "caterpillar\n",
        "chimpanzee\n",
        "cockroach\n",
        "cow\n",
        "coyote\n",
        "crab\n",
        "crow\n",
        "deer\n",
        "dog\n",
        "dolphin\n",
        "donkey\n",
        "dragonfly\n",
        "duck\n",
        "eagle\n",
        "elephant\n",
        "flamingo\n",
        "fly\n",
        "fox\n",
        "goat\n",
        "goldfish\n",
        "goose\n",
        "gorilla\n",
        "grasshopper\n",
        "hamster\n",
        "hare\n",
        "hedgehog\n",
        "hippopotamus\n",
        "hornbill\n",
        "horse\n",
        "hummingbird\n",
        "hyena\n",
        "jellyfish\n",
        "kangaroo\n",
        "koala\n",
        "ladybugs\n",
        "leopard\n",
        "lion\n",
        "lizard\n",
        "lobster\n",
        "mosquito\n",
        "moth\n",
        "mouse\n",
        "octopus\n",
        "okapi\n",
        "orangutan\n",
        "otter\n",
        "owl\n",
        "ox\n",
        "oyster\n",
        "panda\n",
        "parrot\n",
        "pelecaniformes\n",
        "penguin\n",
        "pig\n",
        "pigeon\n",
        "porcupine\n",
        "possum\n",
        "raccoon\n",
        "rat\n",
        "reindeer\n",
        "rhinoceros\n",
        "sandpiper\n",
        "seahorse\n",
        "seal\n",
        "shark\n",
        "sheep\n",
        "snake\n",
        "sparrow\n",
        "squid\n",
        "squirrel\n",
        "starfish\n",
        "swan\n",
        "tiger\n",
        "turkey\n",
        "turtle\n",
        "whale\n",
        "wolf\n",
        "wombat\n",
        "woodpecker\n",
        "zebra"
      ],
      "metadata": {
        "id": "9E34JPUriqBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 우리가 사용하기로 결정한 동물상\n",
        "## bear, cat, dog, deer, donkey, koala, lion,  wolf, cow, fox, goat, horse, kangaroo, mouse, owl, shark, sheep, snake, turtle"
      ],
      "metadata": {
        "id": "cosFwt18lF7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "id": "PYRAvpfiogCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "_P856vR8rrQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import *\n",
        "import wandb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os,cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn import *\n",
        "import torch,torchvision\n",
        "from tqdm import tqdm\n",
        "device = 'cpu'\n",
        "PROJECT_NAME = 'Animal-Clf'"
      ],
      "metadata": {
        "id": "l2A1WoBei58n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install kaggle --upgrade"
      ],
      "metadata": {
        "id": "rG5lDUI-o1M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#api_key_path=\"/content/drive/MyDrive/kaggle.json\""
      ],
      "metadata": {
        "id": "bbfLA3GTrw13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''with open(api_key_path) as f:\n",
        "  api=json.load(f)\n",
        "  os.environ[\"KAGGLE_USERNAME\"]=api[\"username\"]\n",
        "  os.environ[\"KAGGLE_KEY\"]=api[\"key\"]'''"
      ],
      "metadata": {
        "id": "KttRQXfPr6Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!kaggle datasets download -d iamsouravbanerjee/animal-image-dataset-90-different-animals"
      ],
      "metadata": {
        "id": "VKn61SyEomBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!kaggle kernels output programmerrdai/animal-clf -p /path/to/dest"
      ],
      "metadata": {
        "id": "13ydiLmvwdH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=[\"bear\", \"cat\", \"dog\", \"deer\", \"donkey\", \"koala\", \"lion\",  \"wolf\", \"cow\", \"fox\", \"goat\", \"horse\", \"kangaroo\", \"mouse\", \"owl\", \"shark\", \"sheep\", \"snake\", \"turtle\"]"
      ],
      "metadata": {
        "id": "hTZEOlASyG7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Load"
      ],
      "metadata": {
        "id": "-1fSDrR1eEvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "  \n",
        "    labels_l = {}\n",
        "    labels_r = {}\n",
        "    idx = 0\n",
        "    data = []\n",
        "    for folder in os.listdir('/content/drive/MyDrive/Find my lover/animal/animals/animals'):\n",
        "      if folder in labels:\n",
        "        idx += 1\n",
        "        labels_l[folder] = idx\n",
        "        labels_r[idx] = folder\n",
        "    for folder in tqdm(os.listdir('/content/drive/MyDrive/Find my lover/animal/animals/animals')):\n",
        "        if folder in labels:\n",
        "          for file in os.listdir(f'/content/drive/MyDrive/Find my lover/animal/animals/animals/{folder}/'):\n",
        "              img = cv2.imread(f'/content/drive/MyDrive/Find my lover/animal/animals/animals/{folder}/{file}')\n",
        "              img = cv2.resize(img,(56,56))\n",
        "              img = img / 255.0\n",
        "              data.append([\n",
        "                  img,\n",
        "                  np.eye(\n",
        "                      labels_l[folder]+1,len(labels_l)\n",
        "                  )[labels_l[folder]]\n",
        "              ])\n",
        "    X = []\n",
        "    y = []\n",
        "    for d in data:\n",
        "        X.append(d[0])\n",
        "        y.append(d[1])\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,shuffle=False)\n",
        "    X_train = torch.from_numpy(np.array(X_train)).to(device).view(-1,3,56,56).float()\n",
        "    y_train = torch.from_numpy(np.array(y_train)).to(device).float()\n",
        "    X_test = torch.from_numpy(np.array(X_test)).to(device).view(-1,3,56,56).float()\n",
        "    y_test = torch.from_numpy(np.array(y_test)).to(device).float()\n",
        "    return X,y,X_train,X_test,y_train,y_test,labels_l,labels_r,idx,data"
      ],
      "metadata": {
        "id": "bIFU7eYFc8u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Men Data Load"
      ],
      "metadata": {
        "id": "dL90VvY1eR3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_men_data():\n",
        "    data=[]    \n",
        "    folder_path='/content/drive/MyDrive/Find my lover/men/'\n",
        "    for i in range(50):\n",
        "      img_path=str(i+1)+\".jpg\"\n",
        "      img = cv2.imread(folder_path+img_path)\n",
        "      img = cv2.resize(img,(56,56))\n",
        "      img = img / 255.0\n",
        "      data.append(img)\n",
        "                    \n",
        "    men= torch.from_numpy(np.array(data)).to(device).view(-1,3,56,56).float()\n",
        "    \n",
        "    return men"
      ],
      "metadata": {
        "id": "QEJ3q_1q7BFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. loss, accuracy, Returnig Aninal_type of image"
      ],
      "metadata": {
        "id": "rFIHNMr3ecZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(model,X,y,criterion):\n",
        "    preds = model(X)\n",
        "    loss = criterion(preds,y)\n",
        "    return loss.item()\n",
        "def get_accuracy(model,X,y):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    preds = model(X)\n",
        "    for pred,y_batch in zip(preds,y):\n",
        "        pred = int(torch.argmax(pred))\n",
        "        y_batch = int(torch.argmax(y_batch))\n",
        "        if pred == y_batch:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    acc = round(correct/total,3)*100\n",
        "    return acc\n",
        "def get_animal_type(model, X):\n",
        "  pred = model(X)\n",
        "  pred = int(torch.argmax(pred))\n",
        "  return pred"
      ],
      "metadata": {
        "id": "tyvJNY2joD_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model"
      ],
      "metadata": {
        "id": "au7XLiete92m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.max_pool2d = MaxPool2d((2,2),(2,2))\n",
        "        self.activation = ReLU()\n",
        "        self.conv1 = Conv2d(3,7,(5,5))\n",
        "        self.conv1bn = BatchNorm2d(7)\n",
        "        self.conv2 = Conv2d(7,14,(5,5))\n",
        "        self.conv2bn = BatchNorm2d(14)\n",
        "        self.conv3 = Conv2d(14,21,(5,5))\n",
        "        self.conv3bn = BatchNorm2d(21)\n",
        "        self.linear1 = Linear(21*3*3,256)\n",
        "        self.linear1bn = BatchNorm1d(256)\n",
        "        self.linear2 = Linear(256,512)\n",
        "        self.linear2bn = BatchNorm1d(512)\n",
        "        self.linear3 = Linear(512,256)\n",
        "        self.linear3bn = BatchNorm1d(256)\n",
        "        self.output = Linear(256,len(labels))\n",
        "    \n",
        "    def forward(self,X):\n",
        "        preds = self.max_pool2d(self.activation(self.conv1bn(self.conv1(X))))\n",
        "        preds = self.max_pool2d(self.activation(self.conv2bn(self.conv2(preds))))\n",
        "        preds = self.max_pool2d(self.activation(self.conv3bn(self.conv3(preds))))\n",
        "        preds = preds.view(-1,21*3*3)\n",
        "        preds = self.activation(self.linear1bn(self.linear1(preds)))\n",
        "        preds = self.activation(self.linear2bn(self.linear2(preds)))\n",
        "        preds = self.activation(self.linear3bn(self.linear3(preds)))\n",
        "        preds = self.output(preds)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "puDvLliSw3Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. train"
      ],
      "metadata": {
        "id": "j6h7kf6MfkXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y,X_train,X_test,y_train,y_test,labels_l,labels_r,idx,data = load_data()"
      ],
      "metadata": {
        "id": "GQj9ByrageMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model().to(device)\n",
        "criterion=MSELoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "batch_size=32\n",
        "epochs=250"
      ],
      "metadata": {
        "id": "YcXyCJNcxyrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for _ in tqdm(range(epochs)):\n",
        "     for i in range(0,len(X_train),batch_size):\n",
        "         X_batch = X_train[i:i+batch_size]\n",
        "         y_batch = y_train[i:i+batch_size]\n",
        "         model.to(device)\n",
        "         preds = model(X_batch)\n",
        "         loss = criterion(preds,y_batch)\n",
        "         optimizer.zero_grad()\n",
        "         loss.backward()\n",
        "         optimizer.step()\n",
        "         torch.cuda.empty_cache()\n",
        "     torch.cuda.empty_cache()\n",
        "     model.train()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "A-LalU34x5Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. image load for check result"
      ],
      "metadata": {
        "id": "yfxYP075ftou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_list=[]    \n",
        "folder_path='/content/drive/MyDrive/Find my lover/men/'\n",
        "for i in range(50):\n",
        "  img_path=str(i+1)+\".jpg\"\n",
        "  img = cv2.imread(folder_path+img_path)\n",
        "  img_list.append(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n"
      ],
      "metadata": {
        "id": "4JCNZRgxmTwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Value of loss and accuracy"
      ],
      "metadata": {
        "id": "c1U4xBF5f0ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_loss(model,X_train,y_train,criterion)"
      ],
      "metadata": {
        "id": "jNjD2t4hWAw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_loss(model,X_test,y_test,criterion)"
      ],
      "metadata": {
        "id": "ab4bgWgGfJtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(model,X_train, y_train)"
      ],
      "metadata": {
        "id": "U62StYg_WZat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict=model(load_men_data())"
      ],
      "metadata": {
        "id": "2Ghc9ySk8O1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs=plt.subplots(10, 5, figsize=(30, 100))\n",
        "for i in range(10):\n",
        "  for j in range(5):\n",
        "    axs[i, j].imshow(img_list[i*5+j])\n",
        "    axs[i, j].set_title(labels[torch.argmax(predict, axis=1)[i*5+j]], size=20)"
      ],
      "metadata": {
        "id": "NrvTIOu_EPpg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}